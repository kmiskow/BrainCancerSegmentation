{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First steps:\n",
    "1. Download dataset from https://www.kaggle.com/datasets/awsaf49/brats2020-training-data/\n",
    "2. extract data set to /Data/\n",
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BratsDataset(Dataset):\n",
    "    def __init__(self, df_with_paths,batch_size = 8, channel = 0, slices_range = [30,130]):\n",
    "        self.df_with_paths = df_with_paths\n",
    "        self.batch_size = batch_size\n",
    "        self.channel = channel\n",
    "        self.slices_range = slices_range\n",
    "    def __len__(self):\n",
    "        return len(self.df_with_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename  =  self.df_with_paths['BraTS_2020_subject_ID'][idx]\n",
    "        # read 3D imagw\n",
    "        print(os.path.join(filename,os.listdir(filename)[self.channel]))\n",
    "        img3d = np.array(nib.load(os.path.join(filename,os.listdir(filename)[self.channel])).dataobj)\n",
    "        label3d = np.array(nib.load(os.path.join(filename,os.listdir(filename)[1])).dataobj)\n",
    "\n",
    "        \n",
    "        img2d,label2d = self._process(img3d,label3d)\n",
    "        return img2d,label2d\n",
    "\n",
    "    def _process(self,img_3d,label_3d):\n",
    "        '''\n",
    "        Take image 3D and label 3D and return random slice from range \"slices range\"\n",
    "\n",
    "        Optional:\n",
    "        1. Data augmentation\n",
    "        2. normalization\n",
    "        3. other transformations\n",
    "        '''\n",
    "        #generate random z coordinate for slice\n",
    "        z = np.random.randint(self.slices_range[0],self.slices_range[1])  \n",
    "        # return slices\n",
    "        return img_3d[:,:,z].astype(np.float32), label_3d[:,:,z].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split into datasets of size: \n",
      "Train: 369,\n",
      "Test: 63,\n",
      "Valid: 62.\n"
     ]
    }
   ],
   "source": [
    "def generateDatasetPaths(path_to_data,path_training,path_validation):\n",
    "    '''\n",
    "    Generate lists containing folder names for training, tests and validation.\n",
    "    In dataset we already have split between training and validation, \n",
    "    we will split validation dataset into 2 pieces: validation and testing.\n",
    "    Afterwards we are going to shuffle them to gain random order\n",
    "    '''\n",
    "    path_training = path_to_data +'/'+ path_training\n",
    "    path_validation = path_to_data +'/'+ path_validation\n",
    "\n",
    "    train  =  pd.read_csv(path_training + '/name_mapping.csv')\n",
    "    train = train[['BraTS_2020_subject_ID']]\n",
    "    train['BraTS_2020_subject_ID'] = train['BraTS_2020_subject_ID'].apply(lambda x: path_training + '/' + x)\n",
    "    train = train.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    validandtest  =  pd.read_csv(path_validation+'/name_mapping_validation_data.csv')\n",
    "    validandtest = validandtest[['BraTS_2020_subject_ID']]\n",
    "    validandtest['BraTS_2020_subject_ID'] = validandtest['BraTS_2020_subject_ID'].apply(lambda x: path_training + '/' + x)\n",
    "\n",
    "    valid, test = train_test_split(validandtest, test_size=0.5)\n",
    "    valid.reset_index(drop=True,inplace=True)\n",
    "    test.reset_index(drop=True,inplace=True)\n",
    "\n",
    "    print(f'Dataset split into datasets of size: \\nTrain: {len(train)},\\nTest: {len(test)},\\nValid: {len(valid)}.')\n",
    "\n",
    "    return train,test,valid\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01150138\\Desktop\\informatyka mgr\\GSN\\Projekt\\BrainCancerSegmentation\\src\\data\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_filenames,test_filenames,valid_filenames = generateDatasetPaths('../../Brats','BraTS2020_TrainingData','BraTS2020_ValidationData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = BratsDataset(training_filenames)\n",
    "train_dataloader = DataLoader(test1, batch_size=8, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
